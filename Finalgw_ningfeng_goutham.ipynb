{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30881f82-bda9-49e3-9c6e-cfc300a13146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.1\n",
      "GPU: NVIDIA GeForce RTX 5090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b7b0c8-6349-4cf0-ada7-f62843216d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1538305-8e51-41dd-9c15-9fa4bf2ca41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3590bdf-1505-43e9-910f-d4c4f4c23ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bitcoin_tweets.csv',usecols=[\"date\", \"text\"],\n",
    "    low_memory=False,\n",
    "    lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d0b8e0-600b-442f-b3ed-0b59a9629e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4689354, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b936e7b-e9d3-46a9-b7d3-318c58ea245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca12bd2-72a8-41cf-98cc-a7c6e901e111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_44760\\2787545026.py:1: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df['minute'] = df['date'].dt.floor('T')\n"
     ]
    }
   ],
   "source": [
    "df['minute'] = df['date'].dt.floor('T') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ea5843-9070-4bf1-94d7-fb802fd3932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df = df.groupby('minute')['text'].apply(lambda x: ' | '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6b943e8-7b30-4146-9fae-a977ba0621df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13aae350-de84-4dc9-9933-e626136b0569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minute</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-05 10:52:00</td>\n",
       "      <td>#reddcoin #rdd @reddcoin to the moon #altcoin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-05 10:53:00</td>\n",
       "      <td>#Bitcoin and #ETH both have bullish setups for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-05 10:54:00</td>\n",
       "      <td>$PERL 0.06.\\nI have insisted that since 0.02 i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-05 10:57:00</td>\n",
       "      <td>#Amazing üòç\\n#Monopoly #Crypto #cryptocurrency ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-05 10:58:00</td>\n",
       "      <td>#Bitcoin braces for $48,000 as inverse head-an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               minute                                               text\n",
       "0 2021-02-05 10:52:00  #reddcoin #rdd @reddcoin to the moon #altcoin ...\n",
       "1 2021-02-05 10:53:00  #Bitcoin and #ETH both have bullish setups for...\n",
       "2 2021-02-05 10:54:00  $PERL 0.06.\\nI have insisted that since 0.02 i...\n",
       "3 2021-02-05 10:57:00  #Amazing üòç\\n#Monopoly #Crypto #cryptocurrency ...\n",
       "4 2021-02-05 10:58:00  #Bitcoin braces for $48,000 as inverse head-an..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "366834f8-c522-4976-a6e4-4489b80dbbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minute</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>233882</th>\n",
       "      <td>2023-01-09 23:55:00</td>\n",
       "      <td>#BitcoinÔøº is either the most significant inven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233883</th>\n",
       "      <td>2023-01-09 23:56:00</td>\n",
       "      <td>Top 10 coins by 1-week #GalaxyScore‚Ñ¢ via @Luna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233884</th>\n",
       "      <td>2023-01-09 23:57:00</td>\n",
       "      <td>1‚Çø = $17198.76 0.73%üìà\\n\\nDetails:\\nChange: üìà12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233885</th>\n",
       "      <td>2023-01-09 23:58:00</td>\n",
       "      <td>$BTC will come back to life in 2023!\\n\\nDo you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233886</th>\n",
       "      <td>2023-01-09 23:59:00</td>\n",
       "      <td>BECAUSE IT'S A LIFE\\nCHANGING\\nOPPORTUNITY FOR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    minute                                               text\n",
       "233882 2023-01-09 23:55:00  #BitcoinÔøº is either the most significant inven...\n",
       "233883 2023-01-09 23:56:00  Top 10 coins by 1-week #GalaxyScore‚Ñ¢ via @Luna...\n",
       "233884 2023-01-09 23:57:00  1‚Çø = $17198.76 0.73%üìà\\n\\nDetails:\\nChange: üìà12...\n",
       "233885 2023-01-09 23:58:00  $BTC will come back to life in 2023!\\n\\nDo you...\n",
       "233886 2023-01-09 23:59:00  BECAUSE IT'S A LIFE\\nCHANGING\\nOPPORTUNITY FOR..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6550c53c-0841-4ece-9697-58c76d975514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8efc7587-b20f-45d3-a076-552d888aa57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment\"  # 3-class sentiment model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41151abd-226d-409c-941c-63117e2b8f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b07e183-9885-4377-8101-41bbffde714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length',max_length=512).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        scores = softmax(outputs.logits, dim=1).tolist()[0]\n",
    "    \n",
    "    # Map highest probability to sentiment labels\n",
    "    sentiment_mapping = {0: -1, 1: 0, 2: 1}  # Negative = -1, Neutral = 0, Positive = 1\n",
    "    predicted_class = scores.index(max(scores))  # Get index of highest probability\n",
    "    sentiment_score = sentiment_mapping[predicted_class]  # Map to -1, 0, 1\n",
    "    \n",
    "    return sentiment_score, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eae381a2-e7d4-4fa4-89d6-1493ca6f60fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[43mget_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m, in \u001b[0;36mget_sentiment\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m,max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m softmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Map highest probability to sentiment labels\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1320\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1320\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1332\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:912\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    910\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 912\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:99\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;66;03m# Create the position ids from the input token ids. Any padded tokens remain padded.\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_position_ids_from_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    101\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_position_ids_from_inputs_embeds(inputs_embeds)\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1682\u001b[0m, in \u001b[0;36mcreate_position_ids_from_input_ids\u001b[1;34m(input_ids, padding_idx, past_key_values_length)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;124;03mReplace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\u001b[39;00m\n\u001b[0;32m   1674\u001b[0m \u001b[38;5;124;03mare ignored. This is modified from fairseq's `utils.make_positions`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03mReturns: torch.Tensor\u001b[39;00m\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m \u001b[38;5;66;03m# The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\u001b[39;00m\n\u001b[1;32m-> 1682\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mint()\n\u001b[0;32m   1683\u001b[0m incremental_indices \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mcumsum(mask, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(mask) \u001b[38;5;241m+\u001b[39m past_key_values_length) \u001b[38;5;241m*\u001b[39m mask\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m incremental_indices\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;241m+\u001b[39m padding_idx\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "df[['sentiment', 'scores']] = df['text'].apply(lambda x: pd.Series(get_sentiment(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f4adde-6864-499a-be6f-a73a29c2828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assume you have a DataFrame `df` with columns 'date', 'text', and 'sentiment'\n",
    "# where 'sentiment' is a numeric sentiment score (e.g., -1 to 1 or 0 to 10)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df['text']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Option 1: Create a pipeline to chain TF-IDF and Lasso regression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=1000)),\n",
    "    ('lasso', Lasso())\n",
    "])\n",
    "\n",
    "# Optionally, use GridSearchCV to tune the regularization parameter alpha\n",
    "param_grid = {\n",
    "    'lasso__alpha': [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best alpha:\", grid_search.best_params_['lasso__alpha'])\n",
    "print(\"Best CV score (negative MSE):\", grid_search.best_score_)\n",
    "\n",
    "# Option 2: Manual train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the pipeline with a chosen alpha value\n",
    "pipeline.set_params(lasso__alpha=grid_search.best_params_['lasso__alpha'])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test MSE:\", mse)\n",
    "\n",
    "# To inspect the model's coefficients (feature importance):\n",
    "# Get the fitted TfidfVectorizer and Lasso model\n",
    "tfidf = pipeline.named_steps['tfidf']\n",
    "lasso_model = pipeline.named_steps['lasso']\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "coefficients = lasso_model.coef_\n",
    "\n",
    "# Create a DataFrame for easier interpretation\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Display features with non-zero coefficients, sorted by absolute value\n",
    "important_features = coef_df[coef_df['coefficient'] != 0].copy()\n",
    "important_features['abs_coef'] = important_features['coefficient'].abs()\n",
    "important_features = important_features.sort_values(by='abs_coef', ascending=False)\n",
    "print(important_features.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a29efa23-5f9a-4b77-aaf4-f04c4fea5252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb9dd3-40f3-4d4d-a2b2-473ef4242a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db41698-b825-4d20-bd33-03050c440144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2614a29-994a-4370-8562-d6b54da996ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume BTC</th>\n",
       "      <th>Volume USD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1676939580000</td>\n",
       "      <td>2023-02-21 00:33:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24859.34</td>\n",
       "      <td>24859.34</td>\n",
       "      <td>24859.34</td>\n",
       "      <td>24859.34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1676939520000</td>\n",
       "      <td>2023-02-21 00:32:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24821.96</td>\n",
       "      <td>24859.34</td>\n",
       "      <td>24821.96</td>\n",
       "      <td>24859.34</td>\n",
       "      <td>0.103099</td>\n",
       "      <td>2562.977818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1676939460000</td>\n",
       "      <td>2023-02-21 00:31:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24818.09</td>\n",
       "      <td>24821.96</td>\n",
       "      <td>24815.47</td>\n",
       "      <td>24821.96</td>\n",
       "      <td>0.090640</td>\n",
       "      <td>2249.866178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1676939400000</td>\n",
       "      <td>2023-02-21 00:30:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24812.25</td>\n",
       "      <td>24818.09</td>\n",
       "      <td>24812.25</td>\n",
       "      <td>24818.09</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>54.681450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1676939340000</td>\n",
       "      <td>2023-02-21 00:29:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24809.27</td>\n",
       "      <td>24812.25</td>\n",
       "      <td>24809.27</td>\n",
       "      <td>24812.25</td>\n",
       "      <td>0.090675</td>\n",
       "      <td>2249.862431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1676939280000</td>\n",
       "      <td>2023-02-21 00:28:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24809.28</td>\n",
       "      <td>24809.28</td>\n",
       "      <td>24809.27</td>\n",
       "      <td>24809.27</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>98.279938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1676939220000</td>\n",
       "      <td>2023-02-21 00:27:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24809.28</td>\n",
       "      <td>24809.28</td>\n",
       "      <td>24809.28</td>\n",
       "      <td>24809.28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1676939160000</td>\n",
       "      <td>2023-02-21 00:26:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24809.28</td>\n",
       "      <td>24809.28</td>\n",
       "      <td>24809.28</td>\n",
       "      <td>24809.28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1676939100000</td>\n",
       "      <td>2023-02-21 00:25:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24821.31</td>\n",
       "      <td>24821.31</td>\n",
       "      <td>24809.28</td>\n",
       "      <td>24809.28</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>33.758732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1676939040000</td>\n",
       "      <td>2023-02-21 00:24:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24817.20</td>\n",
       "      <td>24821.31</td>\n",
       "      <td>24811.49</td>\n",
       "      <td>24821.31</td>\n",
       "      <td>0.212014</td>\n",
       "      <td>5262.474899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1676938980000</td>\n",
       "      <td>2023-02-21 00:23:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24814.12</td>\n",
       "      <td>24817.20</td>\n",
       "      <td>24814.12</td>\n",
       "      <td>24817.20</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.309470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1676938920000</td>\n",
       "      <td>2023-02-21 00:22:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>24814.12</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>24814.12</td>\n",
       "      <td>0.005529</td>\n",
       "      <td>137.203225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1676938860000</td>\n",
       "      <td>2023-02-21 00:21:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1676938800000</td>\n",
       "      <td>2023-02-21 00:20:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1676938740000</td>\n",
       "      <td>2023-02-21 00:19:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24796.27</td>\n",
       "      <td>24796.27</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>0.007094</td>\n",
       "      <td>175.895842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1676938680000</td>\n",
       "      <td>2023-02-21 00:18:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24797.09</td>\n",
       "      <td>24797.09</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>24796.27</td>\n",
       "      <td>0.027669</td>\n",
       "      <td>686.076092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1676938620000</td>\n",
       "      <td>2023-02-21 00:17:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24799.72</td>\n",
       "      <td>24799.72</td>\n",
       "      <td>24795.61</td>\n",
       "      <td>24797.09</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>2.314560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1676938560000</td>\n",
       "      <td>2023-02-21 00:16:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24818.95</td>\n",
       "      <td>24818.95</td>\n",
       "      <td>24799.72</td>\n",
       "      <td>24799.72</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>48.292743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1676938500000</td>\n",
       "      <td>2023-02-21 00:15:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24823.11</td>\n",
       "      <td>24824.16</td>\n",
       "      <td>24818.95</td>\n",
       "      <td>24818.95</td>\n",
       "      <td>0.006663</td>\n",
       "      <td>165.356751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1676938440000</td>\n",
       "      <td>2023-02-21 00:14:00</td>\n",
       "      <td>BTC/USD</td>\n",
       "      <td>24823.53</td>\n",
       "      <td>24828.31</td>\n",
       "      <td>24817.88</td>\n",
       "      <td>24823.11</td>\n",
       "      <td>0.008618</td>\n",
       "      <td>213.918860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Timestamp                 Date   Symbol      Open      High       Low  \\\n",
       "0   1676939580000  2023-02-21 00:33:00  BTC/USD  24859.34  24859.34  24859.34   \n",
       "1   1676939520000  2023-02-21 00:32:00  BTC/USD  24821.96  24859.34  24821.96   \n",
       "2   1676939460000  2023-02-21 00:31:00  BTC/USD  24818.09  24821.96  24815.47   \n",
       "3   1676939400000  2023-02-21 00:30:00  BTC/USD  24812.25  24818.09  24812.25   \n",
       "4   1676939340000  2023-02-21 00:29:00  BTC/USD  24809.27  24812.25  24809.27   \n",
       "5   1676939280000  2023-02-21 00:28:00  BTC/USD  24809.28  24809.28  24809.27   \n",
       "6   1676939220000  2023-02-21 00:27:00  BTC/USD  24809.28  24809.28  24809.28   \n",
       "7   1676939160000  2023-02-21 00:26:00  BTC/USD  24809.28  24809.28  24809.28   \n",
       "8   1676939100000  2023-02-21 00:25:00  BTC/USD  24821.31  24821.31  24809.28   \n",
       "9   1676939040000  2023-02-21 00:24:00  BTC/USD  24817.20  24821.31  24811.49   \n",
       "10  1676938980000  2023-02-21 00:23:00  BTC/USD  24814.12  24817.20  24814.12   \n",
       "11  1676938920000  2023-02-21 00:22:00  BTC/USD  24795.61  24814.12  24795.61   \n",
       "12  1676938860000  2023-02-21 00:21:00  BTC/USD  24795.61  24795.61  24795.61   \n",
       "13  1676938800000  2023-02-21 00:20:00  BTC/USD  24795.61  24795.61  24795.61   \n",
       "14  1676938740000  2023-02-21 00:19:00  BTC/USD  24796.27  24796.27  24795.61   \n",
       "15  1676938680000  2023-02-21 00:18:00  BTC/USD  24797.09  24797.09  24795.61   \n",
       "16  1676938620000  2023-02-21 00:17:00  BTC/USD  24799.72  24799.72  24795.61   \n",
       "17  1676938560000  2023-02-21 00:16:00  BTC/USD  24818.95  24818.95  24799.72   \n",
       "18  1676938500000  2023-02-21 00:15:00  BTC/USD  24823.11  24824.16  24818.95   \n",
       "19  1676938440000  2023-02-21 00:14:00  BTC/USD  24823.53  24828.31  24817.88   \n",
       "\n",
       "       Close  Volume BTC   Volume USD  \n",
       "0   24859.34    0.000000     0.000000  \n",
       "1   24859.34    0.103099  2562.977818  \n",
       "2   24821.96    0.090640  2249.866178  \n",
       "3   24818.09    0.002203    54.681450  \n",
       "4   24812.25    0.090675  2249.862431  \n",
       "5   24809.27    0.003961    98.279938  \n",
       "6   24809.28    0.000000     0.000000  \n",
       "7   24809.28    0.000000     0.000000  \n",
       "8   24809.28    0.001361    33.758732  \n",
       "9   24821.31    0.212014  5262.474899  \n",
       "10  24817.20    0.000012     0.309470  \n",
       "11  24814.12    0.005529   137.203225  \n",
       "12  24795.61    0.000000     0.000000  \n",
       "13  24795.61    0.000000     0.000000  \n",
       "14  24795.61    0.007094   175.895842  \n",
       "15  24796.27    0.027669   686.076092  \n",
       "16  24797.09    0.000093     2.314560  \n",
       "17  24799.72    0.001947    48.292743  \n",
       "18  24818.95    0.006663   165.356751  \n",
       "19  24823.11    0.008618   213.918860  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prices.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c3caff2-5f45-4e19-bcfb-4269fafb7f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices['Date'] = pd.to_datetime(df_prices['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a75213a-401c-4d3d-b2a4-e1831846c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_shifted = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e93b3af-6266-4ee4-b3b0-6f2847b6456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_shifted['minute'] = df1_shifted['minute'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92cb6856-2ee0-42fe-978f-d91e64ae8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_renamed = df1_shifted.rename(columns={'minute': 'Date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a816b444-07c5-41f6-8c3a-07fbb3856d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(df_prices, df1_renamed, on='Date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e827ee12-4da7-4b14-9609-caa4c0f27acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229044, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d78962e-2921-4d2a-a9a7-be062e4d49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('text_price_comb.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
